#!/usr/bin/env python3 -u
"""Single-file voice dictation with toggle via socket"""
import sys
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)
import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
import queue, socket, os, time, threading, subprocess, sys
import httpx
from dotenv import load_dotenv
import base64
import io
import wave

load_dotenv()

SOCKET_PATH = "/tmp/voice-dictation.sock"
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
USE_AUDIO_DIRECT = os.getenv("USE_AUDIO_DIRECT", "true").lower() == "true"
USE_OPENAI_DIRECT = os.getenv("USE_OPENAI_DIRECT", "false").lower() == "true"

class VoiceDictation:
    def __init__(self):
        if USE_OPENAI_DIRECT:
            self.model = None
            print("Using OpenAI Direct API for audio transcription")
        elif not USE_AUDIO_DIRECT:
            self.model = WhisperModel("small", device="cpu", compute_type="int8")
        else:
            self.model = None
            print("Using audio-direct mode (no Whisper)")
        self.sample_rate = 16000
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.audio_data = []
        
        subprocess.run(['pactl', 'set-source-volume', '@DEFAULT_SOURCE@', '150%'], 
                       stderr=subprocess.DEVNULL)
        
        self.stream = sd.InputStream(
            samplerate=self.sample_rate, channels=1, dtype=np.float32,
            callback=lambda indata, *_: self.audio_queue.put(indata.copy()) if self.is_recording else None
        )
        self.stream.start()
        
        if os.path.exists(SOCKET_PATH): os.remove(SOCKET_PATH)
        self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.socket.bind(SOCKET_PATH)
        self.socket.listen(1)
        os.chmod(SOCKET_PATH, 0o666)
        
        print("Voice Dictation Server Ready")
    
    def toggle(self):
        if self.is_recording:
            self.is_recording = False
            subprocess.run(['notify-send', '-t', '1000', 'â¹ï¸ Stopping...'], stderr=subprocess.DEVNULL)
            time.sleep(0.1)
            
            while not self.audio_queue.empty():
                self.audio_data.append(self.audio_queue.get())
            
            if self.audio_data:
                threading.Thread(target=self.transcribe, daemon=True).start()
        else:
            self.is_recording = True
            self.audio_data = []
            subprocess.run(['notify-send', '-t', '1000', 'ðŸŽ¤ Recording...'], stderr=subprocess.DEVNULL)
    
    def audio_to_base64(self, audio_array):
        audio_int16 = (audio_array * 32767).astype(np.int16)
        
        buffer = io.BytesIO()
        with wave.open(buffer, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(self.sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        buffer.seek(0)
        audio_base64 = base64.b64encode(buffer.read()).decode('utf-8')
        return audio_base64
    
    def normalize_audio(self, audio_array):
        max_val = np.max(np.abs(audio_array))
        if max_val > 0:
            return audio_array / max_val * 0.95
        return audio_array
    
    def save_audio_debug(self, audio_array, filename="/tmp/debug_audio.wav"):
        audio_int16 = (audio_array * 32767).astype(np.int16)
        
        with wave.open(filename, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(self.sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        print(f"Saved debug audio to {filename}")
    
    def transcribe_with_openai_direct(self, audio_array):
        try:
            audio_base64 = self.audio_to_base64(audio_array)
            
            response = httpx.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "gpt-4o-audio-preview",
                    "modalities": ["text"],
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": "Transcribe."
                                },
                                {
                                    "type": "input_audio",
                                    "input_audio": {
                                        "data": audio_base64,
                                        "format": "wav"
                                    }
                                }
                            ]
                        }
                    ],
                },
                timeout=15.0,
            )
            
            if response.status_code == 200:
                result = response.json()
                text = result["choices"][0]["message"]["content"]
                if text:
                    text = text.strip()
                    # Remove common preambles
                    preambles = [
                        "here is the transcription:",
                        "following your rules,",
                        "the transcription is:",
                        "following the formatting rules,",
                        "removing all filler words",
                        "sure. here is the transcribed text:",
                        "the following is the cleaned transcription:",
                        "applying the rules,"
                    ]
                    text_lower = text.lower()
                    for preamble in preambles:
                        if text_lower.startswith(preamble):
                            text = text[len(preamble):].strip()
                            text_lower = text.lower()
                    # Remove leading newlines
                    text = text.lstrip('\n').strip()
                    return text
                else:
                    print(f"OpenAI returned empty content")
                    return None
            else:
                print(f"OpenAI API failed: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            print(f"OpenAI Direct error: {e}")
            return None
    
    def transcribe_with_audio_llm(self, audio_array):
        try:
            print(f"Audio array length: {len(audio_array)}, duration: {len(audio_array)/self.sample_rate:.2f}s")
            audio_base64 = self.audio_to_base64(audio_array)
            print(f"Base64 length: {len(audio_base64)}")
            
            response = httpx.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "mistralai/voxtral-small-24b-2507",
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are a transcription system. Output ONLY the transcribed text. No preambles. No commentary. No explanations. Just the cleaned text."
                        },
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": "Transcribe the following audio clip. Output only the spoken words with proper punctuation and capitalization. Remove filler words like um, uh, and like."
                                },
                                {
                                    "type": "input_audio",
                                    "inputAudio": {
                                        "data": audio_base64,
                                        "format": "wav"
                                    }
                                }
                            ]
                        }
                    ],
                },
                timeout=15.0,
            )
            
            if response.status_code == 200:
                text = response.json()["choices"][0]["message"]["content"].strip()
                return text
            else:
                print(f"Audio LLM failed: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            print(f"Audio LLM error: {e}")
            return None
    
    def clean_with_llm(self, raw_text):
        try:
            response = httpx.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "google/gemini-2.5-flash",
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are a transcription system. Output ONLY the transcribed text. No preambles. No commentary. No explanations. Just the cleaned text."
                        },
                        {
                            "role": "user",
                            "content": f"""This is raw voice-to-text transcription that needs cleanup and formatting.

Instructions:
- Fix grammar, punctuation, and capitalization
- Remove filler words (um, uh, like, you know, etc.)
- If the speaker mentions multiple points using words like "first", "second", "third" OR "one", "two", "three", format them as a numbered list using "1.", "2.", "3." format
- If there are bullet points or lists mentioned, format them properly
- Keep the meaning intact
- Return ONLY the cleaned and formatted text, nothing else

Raw transcription: {raw_text}"""
                        }
                    ],
                },
                timeout=10.0,
            )
            
            if response.status_code == 200:
                cleaned = response.json()["choices"][0]["message"]["content"].strip()
                return cleaned
            else:
                return raw_text
        except Exception as e:
            print(f"LLM cleanup failed: {e}")
            return raw_text
    
    def transcribe(self):
        audio = np.concatenate(self.audio_data, axis=0).flatten()
        audio = self.normalize_audio(audio)
        
        if USE_OPENAI_DIRECT:
            self.save_audio_debug(audio)
            subprocess.run(['notify-send', '-t', '1500', 'ðŸŽ™ï¸ Transcribing with OpenAI...'], stderr=subprocess.DEVNULL)
            text = self.transcribe_with_openai_direct(audio)
            
            if not text:
                subprocess.run(['notify-send', '-t', '2000', 'âŒ Transcription failed'], stderr=subprocess.DEVNULL)
                return
        elif USE_AUDIO_DIRECT:
            self.save_audio_debug(audio)
            subprocess.run(['notify-send', '-t', '1500', 'ðŸŽµ Processing audio with LLM...'], stderr=subprocess.DEVNULL)
            text = self.transcribe_with_audio_llm(audio)
            
            if not text:
                subprocess.run(['notify-send', '-t', '2000', 'âŒ Audio processing failed'], stderr=subprocess.DEVNULL)
                return
        else:
            segments, _ = self.model.transcribe(audio, language="en", beam_size=5, vad_filter=True)
            raw_text = " ".join([s.text for s in segments]).strip()
            
            if not raw_text:
                return
            
            subprocess.run(['notify-send', '-t', '1500', 'ðŸ§¹ Cleaning text...'], stderr=subprocess.DEVNULL)
            text = self.clean_with_llm(raw_text)
        
        if text:
            try:
                subprocess.run(['wtype', text], check=True, stderr=subprocess.DEVNULL)
            except:
                try:
                    subprocess.run(['xdotool', 'type', '--clearmodifiers', text], check=True, stderr=subprocess.DEVNULL)
                except:
                    subprocess.run(['wl-copy', text], stderr=subprocess.DEVNULL)
            subprocess.run(['notify-send', '-t', '2000', f'âœ“ {text[:50]}...'], stderr=subprocess.DEVNULL)
    
    def run(self):
        try:
            while True:
                conn, _ = self.socket.accept()
                cmd = conn.recv(1024).decode().strip()
                print(f"Received command: {cmd}")
                if cmd == "start":
                    if not self.is_recording:
                        self.is_recording = True
                        self.audio_data = []
                        subprocess.run(['notify-send', '-t', '1000', 'ðŸŽ¤ Recording...'], stderr=subprocess.DEVNULL)
                elif cmd == "stop":
                    if self.is_recording:
                        self.is_recording = False
                        subprocess.run(['notify-send', '-t', '1000', 'â¹ï¸ Stopping...'], stderr=subprocess.DEVNULL)
                        time.sleep(0.1)
                        
                        while not self.audio_queue.empty():
                            self.audio_data.append(self.audio_queue.get())
                        
                        if self.audio_data:
                            threading.Thread(target=self.transcribe, daemon=True).start()
                elif cmd == "toggle":
                    self.toggle()
                conn.close()
        except KeyboardInterrupt:
            pass
        finally:
            self.stream.stop()
            self.stream.close()
            self.socket.close()
            if os.path.exists(SOCKET_PATH): os.remove(SOCKET_PATH)

if __name__ == "__main__":
    if len(sys.argv) > 1:
        cmd = sys.argv[1]
        try:
            s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
            s.connect(SOCKET_PATH)
            s.send(cmd.encode())
            s.close()
        except:
            print("Server not running. Start with: voice-dictate")
    else:
        VoiceDictation().run()
